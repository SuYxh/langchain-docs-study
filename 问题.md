问题




1、 createAgent、initChatModel、ChatOpenAI、checkpointer 这些 API 有什么区别？

**答：**
- **`createAgent`**: 用于创建一个预构建的 ReAct Agent，它封装了完整的 Agent 循环逻辑，包括模型调用、工具执行、状态管理等。这是构建 Agent 的高级抽象。
- **`initChatModel`**: 用于初始化一个聊天模型，它是一个通用的模型初始化函数，可以通过 `provider:model` 格式字符串快速切换不同的模型提供商。
- **`ChatOpenAI`**: 是 OpenAI 特定的聊天模型类，用于直接实例化 OpenAI 的模型，提供更细粒度的配置选项。
- **`checkpointer`**: 用于状态持久化，它可以保存 Agent 的状态到数据库，使得对话可以在任何时候恢复。常用的是 `MemorySaver`。

```typescript
// initChatModel - 通用方式
const model = await initChatModel("gpt-4.1");

// ChatOpenAI - OpenAI 特定方式
const model = new ChatOpenAI({ model: "gpt-4.1", apiKey: "your-api-key" });

// createAgent - 创建完整的 Agent
const agent = createAgent({
  model,
  tools: [...],
  checkpointer: new MemorySaver(), // 启用状态持久化
});
```

**initChatModel("google-genai:gemini-2.5-flash-lite") 的含义：**
- 格式是 `provider:model`，其中 `google-genai` 是提供商名称，`gemini-2.5-flash-lite` 是具体的模型名称。
- 这种格式允许你快速切换不同提供商的模型。

**initChatModel 如何设置 apiKey：**
```typescript
// 方式1: 通过环境变量 (推荐)
process.env.OPENAI_API_KEY = "your-api-key";
const model = await initChatModel("gpt-4.1");

// 方式2: 通过参数
const model = await initChatModel("gpt-4.1", {
  modelProvider: "openai",
  apiKey: "your-api-key",
});
```



2、我设置了 responseFormat ，就一定能得到对应的结构吗？ langchain 是如何保证的呢？ 我在代码中设置了 responseFormat， 还需要在系统提示词中要求输出结构为  responseFormat 的格式呢？

**答：**
是的，设置了 `responseFormat` 后，LangChain 会**保证**模型的最终响应符合指定的 schema 结构。

**LangChain 保证结构化输出的机制：**
1. **Provider Strategy**: 如果模型原生支持结构化输出（如 OpenAI、Anthropic），LangChain 会使用模型提供商的 API 直接约束输出格式。
2. **Tool Strategy**: 如果模型不支持原生结构化输出，LangChain 会创建一个工具调用，强制模型通过工具调用的方式返回符合 schema 的数据。
3. **Zod 验证**: 当使用 Zod schema 时，LangChain 会使用 Zod 的 `parse` 方法验证输出，如果不符合会报错或重试。

**不需要在系统提示词中重复要求格式**，因为：
- 模型会自动收到 schema 的信息
- 验证和强制转换是在框架层面完成的

```typescript
import * as z from "zod";
import { createAgent } from "langchain";

const ContactInfo = z.object({
  name: z.string(),
  email: z.string(),
  phone: z.string(),
});

const agent = createAgent({
  model: "gpt-4.1",
  responseFormat: ContactInfo, // 自动保证输出符合此结构
});

const result = await agent.invoke({
  messages: [{ role: "user", content: "Extract: John, john@example.com, 555-1234" }],
});

console.log(result.structuredResponse);
// { name: 'John', email: 'john@example.com', phone: '555-1234' }
```

3、langchain 自动处理了 tool 调用吗？ 就是判断大模型如果返回的是 tool_call 然后调用工具方法

**答：**
**是的，使用 `createAgent` 创建的 Agent 会自动处理 tool 调用循环。**

工作流程：
1. Agent 调用模型生成响应
2. 如果模型返回 `tool_calls`，Agent 自动识别
3. Agent 执行对应的工具函数
4. 工具结果被封装成 `ToolMessage` 返回给模型
5. 模型继续推理，可能再次调用工具或返回最终答案
6. 循环直到模型返回非工具调用的响应

**如果你单独使用 model（不用 Agent）**，则需要手动处理这个循环。

```typescript
// 使用 Agent - 自动处理 tool 调用循环
const agent = createAgent({
  model: "gpt-4.1",
  tools: [getWeather, searchWeb],
});
// 自动调用工具并返回最终结果
const result = await agent.invoke({ messages: [...] });

// 单独使用 model - 需要手动处理
const modelWithTools = model.bindTools([getWeather]);
const response = await modelWithTools.invoke(messages);
if (response.tool_calls?.length) {
  // 需要手动执行工具并循环...
}
```

4、动态工具没有看懂？ 当工具失败时，Agent 将返回带有自定义错误消息

**答：**
这个指的是**工具错误处理**机制。当工具执行失败时，你可以通过 middleware 自定义错误消息返回给模型，让模型有机会修正输入或采取其他策略。

```typescript
import { createAgent, createMiddleware, ToolMessage } from "langchain";

const handleToolErrors = createMiddleware({
  name: "HandleToolErrors",
  wrapToolCall: async (request, handler) => {
    try {
      return await handler(request);
    } catch (error) {
      // 返回自定义错误消息给模型，而不是直接抛出异常
      return new ToolMessage({
        content: `Tool error: Please check your input and try again. (${error})`,
        tool_call_id: request.toolCall.id!,
      });
    }
  },
});

const agent = createAgent({
  model: "gpt-4.1",
  tools: [searchTool, databaseTool],
  middleware: [handleToolErrors],
});
```

这样当工具失败时，模型会收到错误信息并尝试修正，而不是整个 Agent 崩溃。

5、ReAct Agent 的系统提示词有没有需要注意的呢？ 

**答：**
ReAct Agent 的系统提示词注意事项：

1. **简洁明确**: 清楚描述 Agent 的角色和任务
2. **工具使用指导**: 如果有特定的工具使用规则，需要在提示词中说明
3. **Anthropic 特殊要求**: 系统消息必须是第一条消息
4. **避免过长**: 过长的提示词会影响模型性能和成本

```typescript
const agent = createAgent({
  model: "gpt-4.1",
  tools: [search, calculate],
  systemPrompt: "You are a helpful assistant. Be concise and accurate.",
});
```

**最佳实践：**
- 不提供 systemPrompt 时，Agent 会从消息直接推断任务
- 对于复杂场景，可以使用动态系统提示 (middleware)
- 避免在提示词中重复描述工具功能（工具自带 description）

6、看下 Agents 中的 系统提示 (System prompt) 部分， 带有 `{ type: "ephemeral" }` 的 `cache_control` 字段告诉 Anthropic 缓存该内容块，从而减少使用相同系统提示的重复请求的延迟和成本。

**答：**
`cache_control: { type: "ephemeral" }` 是 **Anthropic 特有的提示词缓存功能**。

**作用：**
- 告诉 Anthropic API 缓存该内容块
- 减少重复请求的延迟
- 降低使用成本（特别适合包含大量静态内容的系统提示）

**使用场景：**
- 系统提示中包含大量参考文档
- 多次请求使用相同的系统提示

```typescript
import { createAgent, SystemMessage } from "langchain";

const literaryAgent = createAgent({
  model: "anthropic:claude-sonnet-4-5",
  systemPrompt: new SystemMessage({
    content: [
      {
        type: "text",
        text: "You are an AI assistant tasked with analyzing literary works.",
      },
      {
        type: "text",
        text: "<the entire contents of 'Pride and Prejudice'>", // 大量文本
        cache_control: { type: "ephemeral" } // 缓存此块
      }
    ]
  })
});
```

**注意：** 此功能仅适用于 Anthropic 模型，其他提供商可能不支持。

7、contextSchema 这个是做什么的呢？

**答：**
`contextSchema` 用于定义 **Runtime Context（运行时上下文）** 的类型结构。它允许你在 Agent 调用时传入额外的配置信息，这些信息可以在工具、中间件中访问。

**用途：**
- 传递用户信息（用户 ID、角色等）
- 传递环境配置
- 依赖注入（数据库连接等）

```typescript
import * as z from "zod";
import { createAgent } from "langchain";

const contextSchema = z.object({
  userName: z.string(),
  userRole: z.enum(["admin", "user"]),
});

const agent = createAgent({
  model: "gpt-4.1",
  tools: [...],
  contextSchema, // 定义上下文结构
});

// 调用时传入 context
const result = await agent.invoke(
  { messages: [{ role: "user", content: "What's my name?" }] },
  { context: { userName: "John Smith", userRole: "admin" } }
);
```

**在工具/中间件中访问：**
```typescript
const myMiddleware = createMiddleware({
  wrapModelCall: (request, handler) => {
    const userName = request.runtime.context.userName; // 访问上下文
    return handler(request);
  },
});
```

8、动态系统提示 是只适配当前对话，还是说后续的对话都使用更新后的系统提示词呢？

**答：**
**动态系统提示是瞬态的（Transient）**，只影响当前这一次模型调用，不会持久化到后续对话。

**工作原理：**
- 每次模型调用前，middleware 的 `wrapModelCall` 或 `beforeModel` 会根据当前状态/上下文动态生成系统提示
- 生成的提示词只用于这一次调用
- 下一次调用会重新计算

```typescript
const dynamicPrompt = dynamicSystemPromptMiddleware((state, runtime) => {
  const userRole = runtime.context.userRole;
  if (userRole === "expert") {
    return "You are a helpful assistant. Provide detailed technical responses.";
  }
  return "You are a helpful assistant. Explain concepts simply.";
});
```

**如果你想要持久化更新系统提示**，需要：
1. 将更新存储到 State 或 Store
2. 在动态提示中从存储中读取

9、在 记忆 这个小节出现了  StateSchema  这个 API 有什么用呢？

**答：**
`StateSchema` 用于定义 Agent 的**自定义状态结构**，扩展默认的 `AgentState`（只有 messages）。

**用途：**
- 添加自定义字段到 Agent 状态
- 在 Agent 执行过程中跟踪额外信息
- 支持更复杂的短期记忆需求

```typescript
import { z } from "zod/v4";
import { StateSchema, MessagesValue, createAgent, createMiddleware } from "langchain";

// 定义自定义状态
const CustomState = new StateSchema({
  messages: MessagesValue, // 必须包含 messages
  userId: z.string(),
  preferences: z.record(z.string(), z.any()),
});

// 创建使用自定义状态的中间件
const stateMiddleware = createMiddleware({
  name: "StateExtension",
  stateSchema: CustomState,
});

const agent = createAgent({
  model: "gpt-4.1",
  tools: [],
  middleware: [stateMiddleware],
  checkpointer: new MemorySaver(),
});

// 调用时可以传入自定义状态字段
const result = await agent.invoke({
  messages: [{ role: "user", content: "Hello" }],
  userId: "user_123",
  preferences: { theme: "dark" },
});
```

10、流式输出中 streamMode 有多少种呢？

**答：**
LangGraph 支持 **5 种 streamMode**：

| Mode | 描述 |
|------|------|
| `values` | 流式输出每一步之后的**完整状态** |
| `updates` | 流式输出每一步的**状态增量更新** |
| `messages` | 流式输出 **LLM tokens 和元数据**（适合聊天应用） |
| `custom` | 流式输出**自定义数据**（通过 writer 发送） |
| `debug` | 流式输出**尽可能多的调试信息** |

**使用方式：**
```typescript
// 单一模式
for await (const chunk of await agent.stream(input, { streamMode: "messages" })) {
  console.log(chunk);
}

// 多种模式组合
for await (const [mode, chunk] of await agent.stream(
  input,
  { streamMode: ["updates", "messages", "custom"] }
)) {
  console.log(`${mode}: ${JSON.stringify(chunk)}`);
}
```

11、langchain 的中间件在技术上是如何实现的呢？ 和 koa 框架的 洋葱模型是一样的吗？

**答：**
LangChain 的中间件实现与 Koa 的洋葱模型**有相似之处但不完全相同**。

**LangChain Middleware 的 Hooks：**

| Hook | 执行时机 | 用途 |
|------|---------|------|
| `beforeAgent` | Agent 调用前 | 加载记忆、验证输入 |
| `beforeModel` | 每次 LLM 调用前 | 更新提示词、裁剪消息 |
| `wrapModelCall` | 包裹 LLM 调用 | 拦截和修改请求/响应 |
| `wrapToolCall` | 包裹工具调用 | 拦截和修改工具执行 |
| `afterModel` | 每次 LLM 调用后 | 验证输出、应用护栏 |
| `afterAgent` | Agent 完成后 | 保存结果、清理 |

**与 Koa 洋葱模型的区别：**

1. **Koa 洋葱模型**: 请求依次穿过所有中间件的前半部分，到达最内层后，再依次返回穿过后半部分。
2. **LangChain Middleware**: 更像是**钩子系统**，不同的 hook 在不同的生命周期点被调用，`wrapModelCall` 和 `wrapToolCall` 类似洋葱模型的 wrap 概念。

```typescript
const myMiddleware = createMiddleware({
  name: "MyMiddleware",
  beforeModel: (state) => {
    console.log("Before model call");
  },
  wrapModelCall: async (request, handler) => {
    console.log("Before handler (洋葱外层)");
    const response = await handler(request); // 调用下一层
    console.log("After handler (洋葱外层返回)");
    return response;
  },
  afterModel: (state) => {
    console.log("After model call");
  },
});
```

12、有关特定于提供商的集成信息和功能，请参阅提供商的 [聊天模型页面](/oss/javascript/integrations/chat)。 

**答：**
这是文档中的一个导航提示，指向各个模型提供商的具体集成文档。

访问 https://docs.langchain.com/oss/javascript/integrations/chat 可以查看：
- OpenAI (GPT-4, GPT-4o 等)
- Anthropic (Claude)
- Google (Gemini)
- Azure OpenAI
- Ollama (本地模型)
- 等等

每个提供商页面包含：
- 安装说明
- 配置方式
- 特有功能（如 Anthropic 的 prompt caching）
- 支持的模型列表

13、langchain 支持多模态吗？ 如何输出视频、图片呢？

**答：**
**是的，LangChain 完全支持多模态！**

**输入多模态数据：**
```typescript
import { HumanMessage } from "langchain";

// 图片输入 - URL
const message = new HumanMessage({
  content: [
    { type: "text", text: "Describe this image." },
    { type: "image", source_type: "url", url: "https://example.com/image.jpg" },
  ],
});

// 图片输入 - Base64
const message = new HumanMessage({
  content: [
    { type: "text", text: "Describe this image." },
    { type: "image", source_type: "base64", data: "...", mimeType: "image/jpeg" },
  ],
});

// 视频输入
const message = new HumanMessage({
  content: [
    { type: "text", text: "Describe this video." },
    { type: "video", source_type: "base64", data: "..." },
  ],
});

// 音频输入
const message = new HumanMessage({
  content: [
    { type: "text", text: "Transcribe this audio." },
    { type: "audio", source_type: "base64", data: "..." },
  ],
});
```

**输出多模态数据（图片生成等）：**
某些模型支持生成图片等多模态输出：
```typescript
const response = await model.invoke("Create a picture of a cat");
console.log(response.contentBlocks);
// [
//   { type: "text", text: "Here's a picture of a cat" },
//   { type: "image", data: "...", mimeType: "image/jpeg" },
// ]
```

**注意：** 不是所有模型都支持多模态，需要查看具体提供商的支持情况。

14、stream 流式输出. block.type 是什么意思呢？ model.stream 和 model.streamEvents 有什么区别？

**答：**
**`block.type` 含义：**

`contentBlocks` 中的 `type` 表示内容块的类型：

| type | 含义 |
|------|------|
| `text` | 普通文本内容 |
| `reasoning` | 模型的推理过程（如 o1 模型的思考过程） |
| `tool_call_chunk` | 工具调用的片段（流式传输中） |
| `tool_call` | 完整的工具调用 |
| `image` | 图片内容 |
| `audio` | 音频内容 |
| `video` | 视频内容 |

**`model.stream` vs `model.streamEvents` 区别：**

| 方法 | 返回类型 | 用途 |
|------|---------|------|
| `model.stream()` | `AsyncIterator<AIMessageChunk>` | 流式返回消息块，需要手动处理聚合 |
| `model.streamEvents()` | `AsyncIterator<Event>` | 流式返回语义事件，自动在后台聚合完整消息，更容易根据事件类型过滤 |

```typescript
// model.stream - 返回原始 chunk，需要手动拼接
const stream = await model.stream("Hello");
let full: AIMessageChunk | null = null;
for await (const chunk of stream) {
  full = full ? full.concat(chunk) : chunk;
  console.log(full.text); // 累积的文本
}

// streamEvents - 返回结构化事件，更方便过滤和处理
// 在 Agent 的 stream 模式中，LangChain 会自动处理这个
```

**在 Agent 中更推荐使用 `streamMode: "messages"`**，它会自动处理这些细节。

15、toolChoice 这里没有看懂？当流式传输响应时，工具调用通过 [`ToolCallChunk`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.ToolCallChunk.html) 逐步构建，这块再看下？

**答：**
**toolChoice 的作用：**
`toolChoice` 用于**控制模型是否以及如何调用工具**：

```typescript
// 让模型自由选择是否调用工具（默认行为）
const modelWithTools = model.bindTools([tool_1, tool_2]);

// 强制调用特定工具
const modelWithTools = model.bindTools([tool_1], { toolChoice: "tool_1" });

// 强制调用任意一个工具
const modelWithTools = model.bindTools([tool_1, tool_2], { toolChoice: "any" });
```

**ToolCallChunk 的含义：**
当流式传输时，工具调用的参数是**逐步构建**的，不是一次性返回完整的工具调用。

```typescript
// 流式输出中，tool_call 会被拆分成多个 chunk
for await (const chunk of stream) {
  if (chunk.tool_call_chunks) {
    for (const tc of chunk.tool_call_chunks) {
      console.log(`Tool: ${tc.name}`);   // 可能第一个 chunk 有名字
      console.log(`Args: ${tc.args}`);   // 参数会逐步出现: '{"' -> 'city' -> '":"SF"}'
    }
  }
}

// 聚合获取完整的工具调用
let full: AIMessageChunk | null = null;
for await (const chunk of stream) {
  full = full ? full.concat(chunk) : chunk;
}
console.log(full.tool_calls); // 完整的工具调用
// [{ name: 'get_weather', args: { city: 'SF' }, id: 'call_xxx' }]
```

**ToolCallChunk 结构：**
```typescript
{
  type: "tool_call_chunk",
  name: string | null,      // 工具名称（可能为空）
  args: string | null,      // 部分 JSON 参数
  id: string | null,        // 工具调用 ID
  index: number             // 在多工具调用中的位置
}
```

16、了解一下 zod schema  ，这个是怎么做结构化输出的？ 这个在 models 章节提到过

以下的 describe 方法有什么用呢？

```
const Movie = z.object({
  title: z.string().describe("The title of the movie"), // 电影标题
  year: z.number().describe("The year the movie was released"), // 电影上映年份
  director: z.string().describe("The director of the movie"), // 电影导演
  rating: z.number().describe("The movie's rating out of 10"), // 电影评分（满分 10 分）
});
```

`'jsonSchema'`, `'functionCalling'`, `'jsonMode'`  jsonMode 是什么意思？ 哪些模型支持？

**答：**
**Zod Schema 结构化输出原理：**

1. Zod schema 会被转换成 JSON Schema
2. JSON Schema 被发送给模型 API
3. 模型根据 schema 约束生成输出
4. LangChain 用 Zod 验证输出

**`.describe()` 方法的作用：**
- 为每个字段添加**语义描述**
- 这些描述会被包含在发送给模型的 schema 中
- **帮助模型理解每个字段应该包含什么内容**
- 提高输出准确性

```typescript
const Movie = z.object({
  title: z.string().describe("The title of the movie"), // 模型会知道这是"电影标题"
  year: z.number().describe("The year the movie was released"),
  director: z.string().describe("The director of the movie"),
  rating: z.number().describe("The movie's rating out of 10"),
});
```

**三种结构化输出方法的区别：**

| 方法 | 描述 | 特点 |
|------|------|------|
| `jsonSchema` | 使用提供商的原生 JSON Schema 支持 | 最可靠，输出保证符合 schema |
| `functionCalling` | 通过强制工具调用实现结构化输出 | 兼容性好，大多数模型支持 |
| `jsonMode` | 让模型输出有效 JSON，但 schema 需在提示词中描述 | 较老的方式，可靠性较低 |

**模型支持情况：**
- **OpenAI GPT-4 / o1**: 支持 `jsonSchema`
- **Anthropic Claude**: 支持 `functionCalling`
- **Google Gemini**: 支持 `jsonSchema`
- **大多数模型**: 支持 `functionCalling`

```typescript
// 指定方法
const modelWithStructure = model.withStructuredOutput(Movie, { method: "jsonSchema" });
```

17、langchain 如何加载本地的模型呢？ 

**答：**
LangChain 支持通过 **Ollama** 等工具加载本地模型。

**使用 Ollama 加载本地模型：**

1. **安装 Ollama 并下载模型**
```bash
# 安装 Ollama (https://ollama.ai)
# 下载模型
ollama pull llama3
```

2. **安装 LangChain Ollama 包**
```bash
npm install @langchain/ollama
```

3. **使用本地模型**
```typescript
import { ChatOllama } from "@langchain/ollama";

const model = new ChatOllama({
  model: "llama3",
  baseUrl: "http://localhost:11434", // Ollama 默认地址
});

const response = await model.invoke("Hello!");
```

**使用 initChatModel 方式：**
```typescript
import { initChatModel } from "langchain";

const model = await initChatModel("ollama:llama3");
```

**使用 vLLM 或其他 OpenAI 兼容的本地服务：**
```typescript
const model = await initChatModel("MODEL_NAME", {
  modelProvider: "openai",
  baseUrl: "http://localhost:8000/v1", // vLLM 或其他服务地址
  apiKey: "not-needed",
});
```

18、大量的参考上下文应该放在哪里？ 系统提示词中 还是 用户提示词中呢？

- 必须遵守的应该放在哪里？
- 参考类型的，比如教学方法之类的应该放在哪里？

**答：**
**推荐的上下文放置位置：**

| 内容类型 | 推荐位置 | 原因 |
|---------|---------|------|
| **必须遵守的规则/指令** | 系统提示词 | 系统消息优先级最高，模型会优先遵守 |
| **角色定义、行为准则** | 系统提示词 | 定义模型的"人设" |
| **参考文档、教学材料** | 系统提示词 (使用 cache_control) 或用户消息 | 静态内容放系统提示词可缓存 |
| **动态上下文（用户历史等）** | 用户消息或工具返回 | 根据对话动态变化 |
| **当前任务相关的具体数据** | 用户消息 | 与具体问题直接相关 |

**最佳实践：**

```typescript
// 必须遵守的规则 -> 系统提示词
const agent = createAgent({
  model: "anthropic:claude-sonnet-4-5",
  systemPrompt: new SystemMessage({
    content: [
      {
        type: "text",
        text: `You are a customer service agent. 
RULES (MUST FOLLOW):
- Never disclose internal policies
- Always be polite and helpful
- Escalate to human if unable to resolve`,
      },
      {
        type: "text",
        text: "<大量参考文档内容>",
        cache_control: { type: "ephemeral" } // Anthropic 缓存
      }
    ]
  })
});

// 动态参考上下文 -> 可以通过 middleware 注入
const contextMiddleware = createMiddleware({
  wrapModelCall: (request, handler) => {
    // 根据用户或对话状态注入不同的参考内容
    return handler({
      ...request,
      systemMessage: request.systemMessage.concat("Additional context..."),
    });
  },
});
```

**总结：**
- **必须遵守** → 系统提示词（开头位置）
- **参考材料** → 系统提示词（如果静态）或动态注入
- **具体数据** → 用户消息



19. 使用 SystemMessage 、HumanMessage  和  直接写提示词文本有什么区别呢？ 哪个更好呢？

**答：**
**两种方式的区别：**

| 方式 | 特点 | 适用场景 |
|------|------|---------|
| **字符串文本** | 简单直接，自动转换为对应消息类型 | 简单场景，快速原型 |
| **Message 对象** | 更多控制，支持高级功能 | 需要 cache_control、多模态等 |

**什么时候用字符串：**
```typescript
// 简单场景 - 直接用字符串
const agent = createAgent({
  model: "gpt-4.1",
  systemPrompt: "You are a helpful assistant.",
});

// model.invoke 也可以直接传字符串
const response = await model.invoke("What is AI?");
```

**什么时候用 Message 对象：**
```typescript
// 1. 需要多内容块
const systemPrompt = new SystemMessage({
  content: [
    { type: "text", text: "Instructions..." },
    { type: "text", text: "Reference...", cache_control: { type: "ephemeral" } },
  ]
});

// 2. 需要多模态
const message = new HumanMessage({
  content: [
    { type: "text", text: "Describe this image." },
    { type: "image", source_type: "url", url: "..." },
  ]
});

// 3. 构建对话历史
const conversation = [
  new SystemMessage("You are a translator."),
  new HumanMessage("Translate: Hello"),
  new AIMessage("你好"),
  new HumanMessage("Translate: Goodbye"),
];
```

**哪个更好？**
- **简单场景用字符串**：代码更简洁
- **需要高级功能用 Message 对象**：如缓存、多模态、精确控制
- LangChain 内部会将字符串自动转换为 Message 对象，所以功能上等价



20、 File ID  是从哪里获取的呢？

 // From provider-managed File ID
  const message = new HumanMessage({
    content: [
      { type: "text", text: "Describe the content of this image." },
      { type: "image", source_type: "id", id: "file-abc123" },
    ],
  });

**答：**
**File ID 的来源：**

File ID 是由**模型提供商的文件 API** 返回的，需要先上传文件获取。

**OpenAI 文件上传示例：**
```typescript
import OpenAI from "openai";

const openai = new OpenAI();

// 1. 上传文件获取 File ID
const file = await openai.files.create({
  file: fs.createReadStream("image.jpg"),
  purpose: "assistants", // 或 "vision" 等
});

console.log(file.id); // "file-abc123" - 这就是 File ID

// 2. 使用 File ID
const message = new HumanMessage({
  content: [
    { type: "text", text: "Describe this image." },
    { type: "image", source_type: "id", id: file.id },
  ],
});
```

**Anthropic 文件上传：**
Anthropic 也有类似的文件 API。

**使用场景：**
- 文件较大，多次使用同一文件
- 避免重复传输 base64 数据
- 利用提供商的文件存储

**替代方案（不用 File ID）：**
```typescript
// 使用 URL
{ type: "image", source_type: "url", url: "https://example.com/image.jpg" }

// 使用 Base64
{ type: "image", source_type: "base64", data: "...", mimeType: "image/jpeg" }
```



21. tool 中的 writer 有什么用处呢？

**答：**
`writer` 用于在工具执行过程中**发送实时的自定义流式更新**，让用户可以看到工具执行的进度。

**使用场景：**
- 长时间运行的工具显示进度
- 多步骤操作的状态更新
- 实时反馈（如"正在搜索..."、"已处理 50%"）

**工具中使用 writer：**
```typescript
import { tool, ToolRuntime } from "langchain";
import { z } from "zod";

const analyzeData = tool(
  async ({ dataSource }, config: ToolRuntime) => {
    const writer = config.writer;

    // 发送进度更新
    writer?.({ type: "progress", message: "Connecting...", progress: 0 });
    await fetchData();

    writer?.({ type: "progress", message: "Analyzing...", progress: 50 });
    await processData();

    writer?.({ type: "progress", message: "Complete!", progress: 100 });

    return "Analysis complete";
  },
  {
    name: "analyze_data",
    description: "Analyze data with progress updates",
    schema: z.object({
      dataSource: z.string(),
    }),
  }
);
```

**前端接收更新：**
```typescript
// 使用 streamMode: "custom" 接收 writer 发送的数据
for await (const chunk of await agent.stream(
  { messages: [...] },
  { streamMode: ["updates", "custom"] }
)) {
  // chunk 包含 writer 发送的自定义数据
}

// 或在 React 中使用 onCustomEvent
const stream = useStream({
  assistantId: "agent",
  onCustomEvent: (data) => {
    if (data.type === "progress") {
      setProgress(data.progress);
    }
  },
});
```

**注意：** 使用 `writer` 的工具必须在 LangGraph 执行上下文中调用才能正常工作。
22. 
